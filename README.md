# Automated-Essay-Scoring
Automated Essay Scoring on The Hewlett Foundation dataset on Kaggle

Automated essay scoring (AES) uses specialized computer programs to assign grades to essays written in an educational setting. It is a form of educational assessment and an application of natural language processing. Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades, for example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical classification. Automated essay scoring (AES), the task of employing computer technology to score written text, is one of the most important educational applications of natural language processing (NLP). <br><br>
This area of research began with Pageâ€™s [1966] pioneering work on the Project Essay Grader system and has remained active since then. The vast majority of work on AES has focused on holistic scoring, which summarizes the quality of an essay with a single score. There are at least two reasons for this focus. First, corpora manually annotated with holistic scores are publicly available, facilitating the development of learning-based holistic scoring engines. Second, holistic scoring technologies are commercially valuable: being able to automate the scoring of the millions of essays written for standardized aptitude tests such as the SAT and the GRE every year can save a lot of manual grading effort <br><br>
<b>Data Pre Processing: </b><br>
We collected raw essay data from sources like Kaggle, Google Dataset, and a survey of children aged 10-15, and normalized it by converting text to lowercase and removing extra spaces. Tokenization was done using IndicNLP to break essays into words, followed by stopword removal and stemming/lemmatization to reduce data dimensionality. Features were extracted using Bag-of-Words, n-grams, or Word Embeddings. The data was split into training, validation, and testing sets. Gradient Boosting Regressor (GBR) was selected over other regression algorithms for its suitability with regional data containing outliers, lower computational requirements, and interpretability, achieving high accuracy by combining predictions from multiple weak models and tuning hyperparameters.
<br>
<br>
<b>Working of the Model: </b><br>
Gradient Boosting Regressor (GBR) is a powerful algorithm used in Automated Essay Grading (AEG) systems to predict continuous scores based on essays. It iteratively trains weak decision tree models, improving accuracy by adjusting the weights of misclassified data points and training on the weighted dataset. This iterative process continues until a stopping criterion is met, and final predictions are made by aggregating all models' outputs. GBR is effective for regression tasks, especially with appropriate feature engineering. The model's performance is evaluated using Mean Squared Error (MSE) and Mean Absolute Error (MAE). MSE, which penalizes larger errors more, is useful for data with a wide range of values, while MAE treats all errors equally and is less sensitive to outliers, making it suitable for datasets with outliers.
<br>
<be>
<b>Result: </b><br>
As a result of using Gradient Boosting Regression, the prediction model achieved an accuracy of 96%, indicating that the model was highly accurate and effective at predicting the outcomes for the given data.
